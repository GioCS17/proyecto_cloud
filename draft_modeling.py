# -*- coding: utf-8 -*-
"""draft_modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ndv60OpeEi-QgDYiJZU_AaABe98Ok0uR
"""

!pip freeze > requeriments.txt

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

import h5py
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
#import tensorflow as tf
#import sklearn.metrics
import pickle

from sklearn.preprocessing import OneHotEncoder

from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers, Model, optimizers

"""# Collecting Data"""

filename = "/content/drive/MyDrive/Colab Notebooks/UTEC_Sec_Project/data/VDISC_train.hdf5"
data_train = {}

with h5py.File(filename, "r") as f:
    column_names = list(f.keys())
    for column in column_names:
        data_train[column] = list(f[column])

df_train = pd.DataFrame(data_train)
#df_train = df_train.head(100000) #optional, comment in case you want the whole data
print(df_train.shape)
df_train.head()

df_train_cwe = df_train[df_train["CWE-119"] | df_train["CWE-120"] | df_train["CWE-469"] | df_train["CWE-476"] | df_train["CWE-other"]]

df_train_clean = df_train.drop(df_train_cwe.index)

df_train_cwe.shape[0] / df_train_clean.shape[0] *100

df_train_clean_sample = df_train_clean.sample(df_train_cwe.shape[0], random_state=43) #get clean data, code withou vulnerabilities
df_train = pd.concat([df_train_cwe, df_train_clean_sample]) # combine 
df_train = df_train.sample(frac=1).reset_index(drop=True) #shufle data
df_train

del(data_train) # free up space

filename_val = "/content/drive/MyDrive/Colab Notebooks/UTEC_Sec_Project/data/VDISC_validate.hdf5"
data_val = {}

with h5py.File(filename_val, "r") as f:
    column_names = list(f.keys())
    for column in column_names:
        data_val[column] = list(f[column])

df_val = pd.DataFrame(data_val)
#df_val = df_val.head(10000) #optional, comment in case you want the whole data
print(df_val.shape)
df_val.head()

df_val_cwe = df_val[df_val["CWE-119"] | df_val["CWE-120"] | df_val["CWE-469"] | df_val["CWE-476"] | df_val["CWE-other"]]

df_val_clean = df_val.drop(df_val_cwe.index)

df_val_clean_sample = df_val_clean.sample(df_val_cwe.shape[0], random_state=43) #get clean data, code withou vulnerabilities
df_val = pd.concat([df_val_cwe, df_val_clean_sample]) # combine 
df_val = df_val.sample(frac=1).reset_index(drop=True) #shufle data
df_val

del(data_val) # free up space

from google.colab import drive
drive.mount('/content/drive')

"""# EDA """

df_train.isnull().sum()

X_train = df_train["functionSource"]
y_train = df_train.drop(columns="functionSource")
X_train.shape, y_train.shape

X_val = df_val["functionSource"]
y_val = df_val.drop(columns="functionSource")
X_val.shape, y_val.shape

"""# Preprocessing"""

# Set the global value
WORDS_SIZE=10000
INPUT_SIZE=500
NUM_CLASSES=2
MODEL_NUM=0
EPOCHS=10

"""## Scaling Output (CWE)"""

ohe = OneHotEncoder(drop="if_binary", sparse=False)
y_train_preprocessed = ohe.fit_transform(y_train)
y_train_preprocessed

y_val_preprocessed = ohe.transform(y_val)
y_val_preprocessed

X_train = X_train.apply(lambda x : x.decode("utf-8"))
X_val = X_val.apply(lambda x : x.decode("utf-8"))

"""## Tokenization

## For Training Dataset
"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(list(X_train))
len(tokenizer.word_counts)

tokenizer.num_words = WORDS_SIZE
tokenizer.num_words

list_tokenized_train = tokenizer.texts_to_sequences(X_train)

X_train_pad = pad_sequences(list_tokenized_train,  maxlen=INPUT_SIZE, padding='post', dtype = float)

y_train_preprocessed.shape

y_train=[]
for col in range(5):
    y_train.append(to_categorical(y_train_preprocessed[:,col], num_classes=NUM_CLASSES).astype(np.int64))
len(y_train)

"""## For Validation Dataset"""

list_tokenized_val = tokenizer.texts_to_sequences(X_val)
X_val_pad = pad_sequences(list_tokenized_val,  maxlen=INPUT_SIZE, padding='post', dtype = float)

y_val=[]
for col in range(5):
    y_val.append(to_categorical(y_val_preprocessed[:,col], num_classes=NUM_CLASSES).astype(np.int64))
len(y_val)

"""# Modeling"""

random_weights = np.random.normal(size=(WORDS_SIZE, 13),scale=0.01)

# Must use non-sequential model building to create branches in the output layer
inp_layer = layers.Input(shape=(INPUT_SIZE,))
mid_layers = layers.Embedding(input_dim = WORDS_SIZE,
                                    output_dim = 13,
                                    weights=[random_weights],
                                    input_length = INPUT_SIZE)(inp_layer)
mid_layers = layers.Convolution1D(filters=512, kernel_size=(9), padding='same', activation='relu')(mid_layers)
mid_layers = layers.MaxPool1D(pool_size=5)(mid_layers)
mid_layers = layers.Dropout(0.5)(mid_layers)
mid_layers = layers.Flatten()(mid_layers)
mid_layers = layers.Dense(64, activation='relu')(mid_layers)
mid_layers = layers.Dense(16, activation='relu')(mid_layers)
output1 = layers.Dense(2, activation='softmax')(mid_layers)
output2 = layers.Dense(2, activation='softmax')(mid_layers)
output3 = layers.Dense(2, activation='softmax')(mid_layers)
output4 = layers.Dense(2, activation='softmax')(mid_layers)
output5 = layers.Dense(2, activation='softmax')(mid_layers)
model = Model(inp_layer,[output1,output2,output3,output4,output5])

# Define custom optimizers
adam = optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=1, decay=0.0, amsgrad=False)

## Compile model with metrics
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
print("CNN model built: ")
model.summary()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

class_weights = [{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.}]

history = model.fit(x = X_train_pad,
          y = [y_train[0], y_train[1], y_train[2], y_train[3], y_train[4]],
          validation_data = (X_val_pad, [y_val[0], y_val[1], y_val[2], y_val[3], y_val[4]]),
          epochs = 100,
          batch_size = 128,
          verbose =2,
          callbacks=[callback]
          #class_weight= dict(enumerate(class_weights))
          )

model.save("/content/drive/MyDrive/Colab Notebooks/UTEC_Sec_Project/data/model_clean_v1.hdf5")

new_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/UTEC_Sec_Project/data/model_clean_v1.hdf5')
new_model.summary()



"""# Testing"""



filename_test = "/content/drive/MyDrive/Colab Notebooks/UTEC_Sec_Project/data/VDISC_test.hdf5"
data_test = {}

with h5py.File(filename_test, "r") as f:
    column_names = list(f.keys())
    for column in column_names:
        data_test[column] = list(f[column])

df_test = pd.DataFrame(data_test)
print(df_test.shape)
df_test.head()

