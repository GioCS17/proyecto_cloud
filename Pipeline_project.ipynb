{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38y13drotnXK",
        "outputId": "61184254-57cb-4f29-c0e5-dba30df0c914",
        "tags": [
          "imports"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --user --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a8V8LN9ttJT"
      },
      "outputs": [],
      "source": [
        "!pip install kfp --upgrade "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taqx2u69tnXS",
        "outputId": "3a740145-91a3-46fc-9be2-933ffbef82cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: kfp\n",
            "Version: 1.8.12\n",
            "Summary: KubeFlow Pipelines SDK\n",
            "Home-page: https://github.com/kubeflow/pipelines\n",
            "Author: The Kubeflow Authors\n",
            "Author-email: None\n",
            "License: UNKNOWN\n",
            "Location: /root/.local/lib/python3.7/site-packages\n",
            "Requires: requests-toolbelt, typer, PyYAML, jsonschema, Deprecated, kfp-pipeline-spec, google-cloud-storage, cloudpickle, google-api-python-client, kubernetes, kfp-server-api, google-api-core, tabulate, docstring-parser, absl-py, typing-extensions, strip-hints, google-auth, click, fire, protobuf, uritemplate, pydantic\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "# confirm the kfp sdk\n",
        "!pip show kfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B2CjdRcuRgT"
      },
      "source": [
        "## Import kubeflow pipeline libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8P4-rCDtnXT"
      },
      "outputs": [],
      "source": [
        "import kfp\n",
        "import kfp.components as comp\n",
        "import kfp.dsl as dsl\n",
        "from kfp.components import InputPath, OutputPath\n",
        "from typing import NamedTuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWPLyw_DuzSl"
      },
      "source": [
        "## Kubeflow pipeline component creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98g9LoIcuaPB"
      },
      "source": [
        "Component 1: Download the digits Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGK3hlXdtnXV"
      },
      "outputs": [],
      "source": [
        "# download data step\n",
        "def download_data(download_link: str, data_path: OutputPath(str)):\n",
        "    import zipfile\n",
        "    import sys, subprocess;\n",
        "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"wget\"])\n",
        "    import wget\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    # download files\n",
        "    wget.download(download_link.format(file='train'), f'{data_path}/train_csv.zip')\n",
        "    wget.download(download_link.format(file='test'), f'{data_path}/test_csv.zip')\n",
        "    \n",
        "    with zipfile.ZipFile(f\"{data_path}/train_csv.zip\",\"r\") as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "        \n",
        "    with zipfile.ZipFile(f\"{data_path}/test_csv.zip\",\"r\") as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "    \n",
        "    return(print('Done!'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Yhdk3xudcn"
      },
      "source": [
        "Component 2: load the digits Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEdsQpH2tnXX"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "def load_data(data_path: InputPath(str), \n",
        "              load_data_path: OutputPath(str)):\n",
        "    \n",
        "    # import Library\n",
        "    import sys, subprocess;\n",
        "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
        "    # import Library\n",
        "    import os, pickle;\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    #importing the data\n",
        "    # Data Path\n",
        "    train_data_path = data_path + '/train.csv'\n",
        "    test_data_path = data_path + '/test.csv'\n",
        "\n",
        "    # Loading dataset into pandas \n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "    \n",
        "    # join train and test together\n",
        "    ntrain = train_df.shape[0]\n",
        "    ntest = test_df.shape[0]\n",
        "    all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
        "    print(\"all_data size is : {}\".format(all_data.shape))\n",
        "    \n",
        "    #creating the preprocess directory\n",
        "    os.makedirs(load_data_path, exist_ok = True)\n",
        "    \n",
        "    #Save the combined_data as a pickle file to be used by the preprocess component.\n",
        "    with open(f'{load_data_path}/all_data', 'wb') as f:\n",
        "        pickle.dump((ntrain, all_data), f)\n",
        "    \n",
        "    return(print('Done!'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qtdb4r7KlaL7"
      },
      "source": [
        "Component 3: Preprocess the digits Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKWx98TilaL7"
      },
      "outputs": [],
      "source": [
        "# preprocess data\n",
        "\n",
        "def preprocess_data(load_data_path: InputPath(str), \n",
        "                    preprocess_data_path: OutputPath(str)):\n",
        "    # import Library\n",
        "    import sys, subprocess;\n",
        "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
        "    import os, pickle;\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    import h5py\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pickle\n",
        "    \n",
        "\n",
        "    filename = f\"{load_data_path}/VDISC_train.hdf5\"\n",
        "    data_train = {}\n",
        "\n",
        "    with h5py.File(filename, \"r\") as f:\n",
        "        column_names = list(f.keys())\n",
        "        for column in column_names:\n",
        "            data_train[column] = list(f[column])\n",
        "\n",
        "    df_train = pd.DataFrame(data_train)\n",
        "    df_train_cwe = df_train[df_train[\"CWE-119\"] | df_train[\"CWE-120\"] | df_train[\"CWE-469\"] | df_train[\"CWE-476\"] | df_train[\"CWE-other\"]]\n",
        "\n",
        "    df_train_clean = df_train.drop(df_train_cwe.index)\n",
        "\n",
        "    df_train_clean_sample = df_train_clean.sample(df_train_cwe.shape[0], random_state=43) #get clean data, code withou vulnerabilities\n",
        "    df_train = pd.concat([df_train_cwe, df_train_clean_sample]) # combine \n",
        "    df_train = df_train.sample(frac=1).reset_index(drop=True) #shufle data\n",
        "\n",
        "    filename_val = f\"{load_data_path}/VDISC_validate.hdf5\"\n",
        "    data_val = {}\n",
        "\n",
        "    with h5py.File(filename_val, \"r\") as f:\n",
        "        column_names = list(f.keys())\n",
        "        for column in column_names:\n",
        "            data_val[column] = list(f[column])\n",
        "    df_val = pd.DataFrame(data_val)\n",
        "    df_val_cwe = df_val[df_val[\"CWE-119\"] | df_val[\"CWE-120\"] | df_val[\"CWE-469\"] | df_val[\"CWE-476\"] | df_val[\"CWE-other\"]]\n",
        "    df_val_clean = df_val.drop(df_val_cwe.index)\n",
        "    df_val_clean_sample = df_val_clean.sample(df_val_cwe.shape[0], random_state=43) #get clean data, code withou vulnerabilities\n",
        "    df_val = pd.concat([df_val_cwe, df_val_clean_sample]) # combine \n",
        "    df_val = df_val.sample(frac=1).reset_index(drop=True) #shufle data\n",
        "\n",
        "    X_train = df_train[\"functionSource\"]\n",
        "    y_train = df_train.drop(columns=\"functionSource\")\n",
        "\n",
        "    X_val = df_val[\"functionSource\"]\n",
        "    y_val = df_val.drop(columns=\"functionSource\")\n",
        "\n",
        "    ohe = OneHotEncoder(drop=\"if_binary\", sparse=False)\n",
        "    y_train_preprocessed = ohe.fit_transform(y_train)\n",
        "    y_val_preprocessed = ohe.transform(y_val)\n",
        "\n",
        "    X_train = X_train.apply(lambda x : x.decode(\"utf-8\"))\n",
        "    X_val = X_val.apply(lambda x : x.decode(\"utf-8\"))\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(list(X_train))\n",
        "\n",
        "    WORDS_SIZE=10000\n",
        "    tokenizer.num_words = WORDS_SIZE\n",
        "    list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
        "    X_train_pad = pad_sequences(list_tokenized_train,  maxlen=INPUT_SIZE, padding='post', dtype = float)\n",
        "\n",
        "    list_tokenized_val = tokenizer.texts_to_sequences(X_val)\n",
        "    X_val_pad = pad_sequences(list_tokenized_val,  maxlen=INPUT_SIZE, padding='post', dtype = float)\n",
        "\n",
        "\n",
        "    return(print('Done!'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "XSuVzz22laL9"
      },
      "source": [
        "Component 4: ML modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KGMGMEmtnXZ"
      },
      "outputs": [],
      "source": [
        "def modeling(preprocess_data_path: InputPath(str), \n",
        "            model_path: OutputPath(str)):\n",
        "    \n",
        "    # import Library\n",
        "    import sys, subprocess;\n",
        "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','tensorflow'])\n",
        "    import os, pickle;\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras, optimizers\n",
        "    from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "    from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    inp_layer = layers.Input(shape=(INPUT_SIZE,))\n",
        "    mid_layers = layers.Embedding(input_dim = WORDS_SIZE,\n",
        "                                        output_dim = 13,\n",
        "                                        weights=[random_weights],\n",
        "                                        input_length = INPUT_SIZE)(inp_layer)\n",
        "    mid_layers = layers.Convolution1D(filters=512, kernel_size=(9), padding='same', activation='relu')(mid_layers)\n",
        "    mid_layers = layers.MaxPool1D(pool_size=5)(mid_layers)\n",
        "    mid_layers = layers.Dropout(0.5)(mid_layers)\n",
        "    mid_layers = layers.Flatten()(mid_layers)\n",
        "    mid_layers = layers.Dense(64, activation='relu')(mid_layers)\n",
        "    mid_layers = layers.Dense(16, activation='relu')(mid_layers)\n",
        "    output1 = layers.Dense(2, activation='softmax')(mid_layers)\n",
        "    output2 = layers.Dense(2, activation='softmax')(mid_layers)\n",
        "    output3 = layers.Dense(2, activation='softmax')(mid_layers)\n",
        "    output4 = layers.Dense(2, activation='softmax')(mid_layers)\n",
        "    output5 = layers.Dense(2, activation='softmax')(mid_layers)\n",
        "    model = Model(inp_layer,[output1,output2,output3,output4,output5])\n",
        "\n",
        "    # Define custom optimizers\n",
        "    adam = optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=1, decay=0.0, amsgrad=False)\n",
        "\n",
        "    ## Compile model with metrics\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "      \n",
        "    #saving the model\n",
        "    model.save(f'{model_path}/model.h5')\n",
        "    class_weights = [{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.}]\n",
        "\n",
        "    history = model.fit(x = X_train_pad,\n",
        "          y = [y_train[0], y_train[1], y_train[2], y_train[3], y_train[4]],\n",
        "          validation_data = (X_val_pad, [y_val[0], y_val[1], y_val[2], y_val[3], y_val[4]]),\n",
        "          epochs = 100,\n",
        "          batch_size = 128,\n",
        "          verbose =2,\n",
        "          callbacks=[callback]\n",
        "          #class_weight= dict(enumerate(class_weights))\n",
        "          )    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwXJuoHQui3d"
      },
      "source": [
        "Component 5: Prediction and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyyFqzjZlaL_"
      },
      "outputs": [],
      "source": [
        "def prediction(model_path: InputPath(str), \n",
        "                preprocess_data_path: InputPath(str), \n",
        "                mlpipeline_ui_metadata_path: OutputPath(str)) -> NamedTuple('conf_m_result', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
        "    \n",
        "    # import Library\n",
        "    import sys, subprocess;\n",
        "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install','tensorflow'])\n",
        "    import pickle, json;\n",
        "    import pandas as  pd\n",
        "    import numpy as np\n",
        "    from collections import namedtuple\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from tensorflow.keras.models import load_model\n",
        "\n",
        "    #loading the X_test and y_test\n",
        "    with open(f'{preprocess_data_path}/test', 'rb') as f:\n",
        "        test_data = pickle.load(f)\n",
        "    # Separate the X_test from y_test.\n",
        "    X_test, y_test = test_data\n",
        "    \n",
        "    #loading the model\n",
        "    model = load_model(f'{model_path}/model.h5')\n",
        "    \n",
        "    # prediction\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "    \n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    vocab = list(np.unique(y_test))\n",
        "    \n",
        "    # confusion_matrix pair dataset \n",
        "    data = []\n",
        "    for target_index, target_row in enumerate(cm):\n",
        "        for predicted_index, count in enumerate(target_row):\n",
        "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
        "    \n",
        "    # convert confusion_matrix pair dataset to dataframe\n",
        "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
        "    \n",
        "    # change 'target', 'predicted' to integer strings\n",
        "    df[['target', 'predicted']] = (df[['target', 'predicted']].astype(int)).astype(str)\n",
        "    \n",
        "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
        "        json.dump(metadata, metadata_file)\n",
        "\n",
        "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
        "    \n",
        "    return conf_m_result(json.dumps(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vtwxTctlaMA"
      },
      "outputs": [],
      "source": [
        "# create light weight components\n",
        "download_op = comp.create_component_from_func(download_data,base_image=\"python:3.7.1\")\n",
        "load_op = comp.create_component_from_func(load_data,base_image=\"python:3.7.1\")\n",
        "preprocess_op = comp.create_component_from_func(preprocess_data,base_image=\"tensorflow/tensorflow:latest\")\n",
        "comp.create_component_from_func(preprocess_data,)\n",
        "modeling_op = comp.create_component_from_func(modeling, base_image=\"tensorflow/tensorflow:latest\")\n",
        "predict_op = comp.create_component_from_func(prediction, base_image=\"tensorflow/tensorflow:latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "gfdKpmwtlaMA"
      },
      "source": [
        "Create kubeflow pipeline components from images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3bNFpBOuuG-"
      },
      "source": [
        "## Kubeflow pipeline creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECZRaIgCtnXd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define pipeline\n",
        "@dsl.pipeline(name=\"vulnerabilities-detection-pipeline\", \n",
        "              description=\"Performs Preprocessing, training and prediction of vulnerabilities in C/C++ code\")\n",
        "\n",
        "# Define parameters to be fed into pipeline\n",
        "def vulnerabilities_recognizer_pipeline(download_link: str,\n",
        "                             data_path: str,\n",
        "                             load_data_path: str, \n",
        "                             preprocess_data_path: str,\n",
        "                             model_path:str\n",
        "                            ):\n",
        "\n",
        "\n",
        "    # Create download container.\n",
        "    download_container = download_op(download_link)\n",
        "    # Create load container.\n",
        "    load_container = load_op(download_container.output)\n",
        "    # Create preprocess container.\n",
        "    preprocess_container = preprocess_op(load_container.output)\n",
        "    # Create modeling container.\n",
        "    modeling_container = modeling_op(preprocess_container.output)\n",
        "    # Create prediction container.\n",
        "    predict_container = predict_op(modeling_container.output, preprocess_container.output)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq2M3chhtnXd"
      },
      "outputs": [],
      "source": [
        "pipeline_func = vulnerabilities_recognizer_pipeline\n",
        "\n",
        "experiment_name = 'vulnerabilities_recognizer'\n",
        "run_name = pipeline_func.__name__ + ' run'\n",
        "\n",
        "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
        "kfp.compiler.Compiler().compile(pipeline_func,  '{}.zip'.format(experiment_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HxC5GfSlaME"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Vi_GhmlaME"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Pipeline_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kubeflow_notebook": {
      "autosnapshot": true,
      "docker_image": "gcr.io/arrikto/jupyter-kale-py36@sha256:dd3f92ca66b46d247e4b9b6a9d84ffbb368646263c2e3909473c3b851f3fe198",
      "experiment": {
        "id": "6f6c9b81-54e3-414b-974a-6fe8b445a59e",
        "name": "digit_recognize_lightweight"
      },
      "experiment_name": "digit_recognize_lightweight",
      "katib_metadata": {
        "algorithm": {
          "algorithmName": "grid"
        },
        "maxFailedTrialCount": 3,
        "maxTrialCount": 12,
        "objective": {
          "objectiveMetricName": "",
          "type": "minimize"
        },
        "parallelTrialCount": 3,
        "parameters": []
      },
      "katib_run": false,
      "pipeline_description": "Performs Preprocessing, training and prediction of digits",
      "pipeline_name": "digit-recognizer-kfp",
      "snapshot_volumes": true,
      "steps_defaults": [
        "label:access-ml-pipeline:true",
        "label:access-rok:true"
      ],
      "volume_access_mode": "rwm",
      "volumes": [
        {
          "annotations": [],
          "mount_point": "/home/jovyan",
          "name": "arikkto-workspace-7xzjm",
          "size": 5,
          "size_type": "Gi",
          "snapshot": false,
          "type": "clone"
        }
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}